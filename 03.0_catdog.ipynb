{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true)\n",
    "***\n",
    "# *Practicum AI:* CNN - Cat / Dog\n",
    "\n",
    "This exercise adapted from Baig et al. (2020) <i>The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Exercise 3.03, page 130).\n",
    "\n",
    "(20 Minutes)\n",
    "\n",
    "#### Introduction\n",
    "In this exercise, we will create a deep learning convolutional neural network model to classify cats and dogs.  Actually, this is the first exercise in a two-part sequence.  In next week's workshop, you will increase the size of the training dataset, in a process called **data augmentation**.\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"images/02.2_cat_image.jpg\" width=\"250\" height=\"250\" />\n",
    "  <img src=\"images/02.2_dog_image.jpg\" width=\"225\" height=\"225\"/> \n",
    "</p>\n",
    "\n",
    "#### 1. Import packages for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a link to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://github.com/PacktWorkshops/The-Deep-Learning-Workshop/raw/master/Chapter03/Datasets/Exercise3.03/cats_and_dogs_filtered.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Download the dataset\n",
    "\n",
    "Download the dataset to the data folder. After running this cell, explore the `data/datasets/cats_and_dogs_filtered folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dir = tf.keras.utils.get_file('cats_and_dogs.zip', origin = file_url, extract = True, cache_dir='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create a path variable to cats_and_dogs_filtered directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(pathlib.Path(zip_dir).parent, 'cats_and_dogs_filtered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Create path variables to the train and validation directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(path, 'train')\n",
    "validation_dir = os.path.join(path, 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Create path variables to the other directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir,'dogs')\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Create variables to hold train & validation image counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train = len(os.listdir(train_cats_dir)) + len(os.listdir(train_dogs_dir))\n",
    "total_val = len(os.listdir(validation_cats_dir)) + len(os.listdir(validation_dogs_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Instantiate two ImageDataGenerator classes\n",
    "\n",
    "Note: The data in these two objects will be rescaled (standardized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_generator = ImageDataGenerator(rescale = 1./255)\n",
    "validation_image_generator = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Assign values to three parameter variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "img_height = 100\n",
    "img_width  = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Create the training data generator\n",
    "\n",
    "Note that we are using the `image_generator` here to get the images from the folders. No data augmentation is being done at this stage. We will do that in our next session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(batch_size  = batch_size,\n",
    "                                                           directory   = train_dir,\n",
    "                                                           shuffle     = True,\n",
    "                                                           target_size = (img_height, img_width),\n",
    "                                                           class_mode  = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Create the validation data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size  = batch_size,\n",
    "                                                              directory   = validation_dir,\n",
    "                                                              target_size = (img_height, img_width),\n",
    "                                                              class_mode  = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Set random seeds for repeatability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)\n",
    "tf.random.set_seed(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Create our model\n",
    "\n",
    "* The model starts with a convolutional layer. In this layer, we feature 64 3x3 kernels. We define the imput shape with the image dimensions set above and three chanels for color images.\n",
    "* Next, there is a MaxPooling layer\n",
    "* Followed by another convolutional layer, this time with 126 kernels to learn.\n",
    "* Another MaxPooling layer\n",
    "* Then we flatten the images into a long vector for a Dense layer with 128 neurons\n",
    "* That feeds into the final layer with a single neuron and a sigmoid activation--this is a binary classification (cat or dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(64, 3, activation='relu', input_shape=(img_height, img_width ,3)),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(128, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Set the omptimizer and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. Display the model summary\n",
    "\n",
    "Notice the numbers of parameters. A single dense layer for a 100X100X3 image would have 3001 parameters, yet the CNN convolutional layer, even with 64 kernels has only 1,792! The vast majority of our parameters still come from the dense layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 98, 98, 64)        1792      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 49, 49, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 47, 47, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 23, 23, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 67712)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               8667264   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,743,041\n",
      "Trainable params: 8,743,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 14:58:42.704571: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8201\n",
      "2022-09-14 14:58:43.458746: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Running ptxas --version returned 32512\n",
      "2022-09-14 14:58:43.495018: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: ptxas exited with non-zero error code 32512, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2022-09-14 14:58:44.390833: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 10s 58ms/step - loss: 0.7204 - accuracy: 0.5275 - val_loss: 0.6778 - val_accuracy: 0.5817\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 0.6727 - accuracy: 0.6025 - val_loss: 0.6529 - val_accuracy: 0.6431\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 0.6307 - accuracy: 0.6470 - val_loss: 0.6741 - val_accuracy: 0.5887\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 0.5593 - accuracy: 0.7130 - val_loss: 0.7072 - val_accuracy: 0.6542\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 0.4399 - accuracy: 0.7815 - val_loss: 0.7167 - val_accuracy: 0.6522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b909dd32130>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=5,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad! Our fairly simple model can classify cats and dogs with an accuracy of about 80% on the traning data and almost 70% on the validation data after only 5 epochs. The much lower validation accuracy is an indication of overfitting. Remember there are only 2000 images total in the training dataset.\n",
    "\n",
    "***\n",
    "## *Practicum AI:* CNN - Cat/Dog Data Augmentation\n",
    "\n",
    "As is often the case in reality, we are limited in the amount of data for model training. To help get around this limitation, we rely on **data augmentation** to generate transformations of our training data, to create \"new\" imges to train on.  By rotating, flipping, color shifting, etc. our original images, we can increase the diversity of images of cats and dogs that our model sees and improve its performance without needing to get more images to train on.\n",
    "\n",
    "When applying data augmentation, it is important to consider what augmentations are or are not appropriate for your use case. For example, a classifier for red roses and blue violets might not do as well if we employ color shifting in the augmentation!\n",
    "\n",
    "#### 18. Define new data generators\n",
    "\n",
    "We only do data augmenation on training data. Here, we redefine the ImageDataGenerator in step 8 to include data augmentation on the training data, but keep the same ImageDataGenerator used above for the validation_image_generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_generator  = ImageDataGenerator(\n",
    "    rescale = 1 / 255.0,\n",
    "    rotation_range = 20,      # Randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.1,         # Randomly zoom image\n",
    "    width_shift_range = 0.1,  # Randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range = 0.1, # Randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip =True,    # Randomly flip images horizontally\n",
    "    vertical_flip =  False,   # Don't randomly flip images vertically\n",
    ")  \n",
    "\n",
    "validation_image_generator  = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19 Setup the new data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(batch_size  = batch_size,\n",
    "                                                           directory   = train_dir,\n",
    "                                                           shuffle     = True,\n",
    "                                                           target_size = (img_height, img_width),\n",
    "                                                           class_mode  = 'binary')\n",
    "\n",
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size  = batch_size,\n",
    "                                                              directory   = validation_dir,\n",
    "                                                              target_size = (img_height, img_width),\n",
    "                                                              class_mode  = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20. Reset the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)\n",
    "tf.random.set_seed(8)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(64, 3, activation='relu', input_shape=(img_height, img_width ,3)),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(128, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21. Display the model summary\n",
    "\n",
    "```python\n",
    "model.summary()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22. Train the model\n",
    "\n",
    "```python\n",
    "model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch = total_train // batch_size,\n",
    "    epochs = 5,\n",
    "    validation_data = val_data_gen,\n",
    "    validation_steps = total_val // batch_size\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
